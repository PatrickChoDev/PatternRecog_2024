{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train_df = pd.read_csv(train_url) #training set\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test_df = pd.read_csv(test_url) #test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()\n",
    "# So Age, Cabin and Embarked have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "test_df.info()\n",
    "# So we will have to impute the missing values\n",
    "# Seem like Cabin missing TOO MUCH values, so we will drop it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill values with median !!FROM TRAINING DATA!!\n",
    "train_df[\"Age\"] = train_df[\"Age\"].fillna(train_df[\"Age\"].median())\n",
    "test_df[\"Age\"] = test_df[\"Age\"].fillna(train_df[\"Age\"].median())\n",
    "\n",
    "# T8: Median age of training data is\n",
    "train_df[\"Age\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill values with most common value !!FROM TRAIN_dfING DATA!!\n",
    "train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].value_counts().idxmax())\n",
    "test_df[\"Embarked\"] = test_df[\"Embarked\"].fillna(train_df[\"Embarked\"].value_counts().idxmax())\n",
    "\n",
    "# T9: Most common port of embarked is\n",
    "train_df[\"Embarked\"].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare and PClass seems to be correlated, so we will use PClass to impute Fare\n",
    "test_df[\"Fare\"] = test_df[\"Fare\"].fillna(train_df.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_categories = dict([(k,i) for i,k in enumerate(train_df[\"Embarked\"].astype('category').cat.categories.tolist())])\n",
    "train_df[\"EmbarkedClass\"] = train_df[\"Embarked\"].map(embarked_categories)\n",
    "test_df[\"EmbarkedClass\"] = test_df[\"Embarked\"].map(embarked_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_categories = dict([(k,i) for i,k in enumerate(train_df[\"Sex\"].astype('category').cat.categories.tolist())])\n",
    "train_df[\"SexClass\"] = train_df[\"Sex\"].map(sex_categories)\n",
    "test_df[\"SexClass\"] = test_df[\"Sex\"].map(sex_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PassengerId        0\n",
       " Survived           0\n",
       " Pclass             0\n",
       " Name               0\n",
       " Sex                0\n",
       " Age                0\n",
       " SibSp              0\n",
       " Parch              0\n",
       " Ticket             0\n",
       " Fare               0\n",
       " Cabin            687\n",
       " Embarked           0\n",
       " EmbarkedClass      0\n",
       " SexClass           0\n",
       " dtype: int64,\n",
       " PassengerId        0\n",
       " Pclass             0\n",
       " Name               0\n",
       " Sex                0\n",
       " Age                0\n",
       " SibSp              0\n",
       " Parch              0\n",
       " Ticket             0\n",
       " Fare               0\n",
       " Cabin            327\n",
       " Embarked           0\n",
       " EmbarkedClass      0\n",
       " SexClass           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum(),test_df.isna().sum()\n",
    "\n",
    "# Data is quite cleaned!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  1., 22.,  2.],\n",
       "       [ 1.,  0., 38.,  0.],\n",
       "       [ 3.,  0., 26.,  2.],\n",
       "       ...,\n",
       "       [ 3.,  0., 28.,  2.],\n",
       "       [ 1.,  1., 26.,  0.],\n",
       "       [ 3.,  1., 32.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array(train_df[[\"Pclass\",\"SexClass\",\"Age\",\"EmbarkedClass\"]].values,dtype=np.float32).reshape(-1,4)\n",
    "train_label = np.array(train_df[\"Survived\"].values,dtype=np.int8).reshape(-1,1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3. ,  1. , 34.5,  1. ],\n",
       "       [ 3. ,  0. , 47. ,  2. ],\n",
       "       [ 2. ,  1. , 62. ,  1. ],\n",
       "       ...,\n",
       "       [ 3. ,  1. , 38.5,  2. ],\n",
       "       [ 3. ,  1. , 28. ,  2. ],\n",
       "       [ 3. ,  1. , 28. ,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array(test_df[[\"Pclass\",\"SexClass\",\"Age\",\"EmbarkedClass\"]].values,dtype=np.float32)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3cc55bf0b0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(x,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def  __init__(self,train_x,train_y,test_x,learning_rate=1e-3) -> None:\n",
    "    self.train_x = train_x\n",
    "    self.train_y = train_y\n",
    "    self.pred = np.zeros_like(train_y)\n",
    "    self.pred_class = np.zeros_like(train_y)\n",
    "    self.test_x = test_x\n",
    "    self.cost,self.grad = self.cost_function()\n",
    "    self.learning_rate = learning_rate\n",
    "    self.W = self.__random_init_param()\n",
    "    self.test_accuracies = []\n",
    "\n",
    "  def __random_init_param(self):\n",
    "    #size of X is [m, n] where m=sample, n=features\n",
    "    self.train_x = self.__add_bias(self.train_x)\n",
    "    W = np.random.randn(len(self.train_x[0]), 1) # +1 for Bias term\n",
    "    return W\n",
    "  \n",
    "  def __sigmoid(self,z): return 1/(1+np.exp(-z))\n",
    "\n",
    "  def __add_bias(self,X):\n",
    "    Bias = np.ones((len(X), 1))\n",
    "    res = np.concatenate((Bias, X), axis=1)\n",
    "    return res\n",
    "  \n",
    "  def cost_function(self):\n",
    "    if type(self.pred) == \"None\" : return None\n",
    "    m = len(self.train_y)\n",
    "    loss = np.dot(-self.train_y.T, np.log(self.__sigmoid(self.pred)+1e-10))-np.dot((1-self.train_y).T, np.log((1-self.__sigmoid(self.pred)+1e-10)))\n",
    "    cost = (1/m) * loss \n",
    "    grad = (1/m) * (np.dot(self.train_x.T, (self.__sigmoid(self.pred)-self.train_y)))\n",
    "    return cost.astype('float64'), grad.astype('float64')\n",
    "\n",
    "  def predict(self, X=None,sigmoid=True):\n",
    "    if X is None : X = self.train_x\n",
    "    h = np.dot(X, self.W)\n",
    "    return self.__sigmoid(h) if sigmoid else h\n",
    "  \n",
    "  def train_accuracy(self):\n",
    "    return np.squeeze(np.squeeze((sum(self.train_y == self.pred_class)/len(self.train_x))*100))\n",
    "  \n",
    "\n",
    "  def step(self):\n",
    "    #update parameters\n",
    "    self.W = self.W - self.learning_rate * self.grad\n",
    "    self.grad = 0\n",
    "  \n",
    "  def train(self,epoch=100,interuption_step=10,logging_step=None):\n",
    "    if logging_step is None : logging_step = epoch**0.5\n",
    "    loss_step = 0\n",
    "    best_W = self.W\n",
    "    for i in range(1,epoch+1):\n",
    "      self.pred = self.predict(None,False)\n",
    "      self.pred_class = np.where(self.__sigmoid(self.pred) >= 0.5, 1, 0)\n",
    "      cost, grad = self.cost_function()\n",
    "      self.grad = grad\n",
    "      self.step()\n",
    "      if i%logging_step == 0: print(f\"Epoch {i}/{epoch} : Train accuracy {self.train_accuracy()}%\")\n",
    "      if cost.item() < self.cost.item() : \n",
    "        loss_step = 0\n",
    "        best_W = self.W\n",
    "      else :\n",
    "        loss_step += 1\n",
    "        if loss_step == interuption_step :\n",
    "          print(f\"Loss is increasing, stop training at epoch {i}\")\n",
    "          self.W = best_W\n",
    "          break\n",
    "      self.cost = cost\n",
    "  \n",
    "  def test(self):\n",
    "    self.test_pred = self.predict(self.__add_bias(self.test_x))\n",
    "    return self.test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = LogisticRegression(train_data,train_label,test_data,0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000000 : Train accuracy 70.03367003367003%\n",
      "Epoch 2000/1000000 : Train accuracy 75.98204264870931%\n",
      "Epoch 3000/1000000 : Train accuracy 76.31874298540964%\n",
      "Epoch 4000/1000000 : Train accuracy 76.87991021324355%\n",
      "Epoch 5000/1000000 : Train accuracy 77.77777777777779%\n",
      "Epoch 6000/1000000 : Train accuracy 78.45117845117845%\n",
      "Epoch 7000/1000000 : Train accuracy 78.45117845117845%\n",
      "Epoch 8000/1000000 : Train accuracy 78.33894500561168%\n",
      "Epoch 9000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 10000/1000000 : Train accuracy 78.00224466891133%\n",
      "Epoch 11000/1000000 : Train accuracy 78.33894500561168%\n",
      "Epoch 12000/1000000 : Train accuracy 78.33894500561168%\n",
      "Epoch 13000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 14000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 15000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 16000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 17000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 18000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 19000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 20000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 21000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 22000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 23000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 24000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 25000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 26000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 27000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 28000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 29000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 30000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 31000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 32000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 33000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 34000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 35000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 36000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 37000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 38000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 39000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 40000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 41000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 42000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 43000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 44000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 45000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 46000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 47000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 48000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 49000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 50000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 51000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 52000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 53000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 54000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 55000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 56000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 57000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 58000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 59000/1000000 : Train accuracy 78.78787878787878%\n",
      "Epoch 60000/1000000 : Train accuracy 78.78787878787878%\n",
      "Epoch 61000/1000000 : Train accuracy 78.78787878787878%\n",
      "Epoch 62000/1000000 : Train accuracy 78.78787878787878%\n",
      "Epoch 63000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 64000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 65000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 66000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 67000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 68000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 69000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 70000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 71000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 72000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 73000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 74000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 75000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 76000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 77000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 78000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 79000/1000000 : Train accuracy 79.68574635241302%\n",
      "Epoch 80000/1000000 : Train accuracy 79.68574635241302%\n",
      "Epoch 81000/1000000 : Train accuracy 79.7979797979798%\n",
      "Epoch 82000/1000000 : Train accuracy 79.7979797979798%\n",
      "Epoch 83000/1000000 : Train accuracy 79.7979797979798%\n",
      "Epoch 84000/1000000 : Train accuracy 79.91021324354658%\n",
      "Epoch 85000/1000000 : Train accuracy 79.91021324354658%\n",
      "Epoch 86000/1000000 : Train accuracy 79.91021324354658%\n",
      "Epoch 87000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 88000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 89000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 90000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 91000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 92000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 93000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 94000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 95000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 96000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 97000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 98000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 99000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 100000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 101000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 102000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 103000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 104000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 105000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 106000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 107000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 108000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 109000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 110000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 111000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 112000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 113000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 114000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 115000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 116000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 117000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 118000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 119000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 120000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 121000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 122000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 123000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 124000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 125000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 126000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 127000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 128000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 129000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 130000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 131000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 132000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 133000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 134000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 135000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 136000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 137000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 138000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 139000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 140000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 141000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 142000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 143000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 144000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 145000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 146000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 147000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 148000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 149000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 150000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 151000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 152000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 153000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 154000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 155000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 156000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 157000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 158000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 159000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 160000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 161000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 162000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 163000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 164000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 165000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 166000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 167000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 168000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 169000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 170000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 171000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 172000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 173000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 174000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 175000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 176000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 177000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 178000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 179000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 180000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 181000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 182000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 183000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 184000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 185000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 186000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 187000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 188000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 189000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 190000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 191000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 192000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 193000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 194000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 195000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 196000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 197000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 198000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 199000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 200000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 201000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 202000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 203000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 204000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 205000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 206000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 207000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 208000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 209000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 210000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 211000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 212000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 213000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 214000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 215000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 216000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 217000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 218000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 219000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 220000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 221000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 222000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 223000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 224000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 225000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 226000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 227000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 228000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 229000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 230000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 231000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 232000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 233000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 234000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 235000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 236000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 237000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 238000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 239000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 240000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 241000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 242000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 243000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 244000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 245000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 246000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 247000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 248000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 249000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 250000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 251000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 252000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 253000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 254000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 255000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 256000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 257000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 258000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 259000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 260000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 261000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 262000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 263000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 264000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 265000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 266000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 267000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 268000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 269000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 270000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 271000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 272000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 273000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 274000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 275000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 276000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 277000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 278000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 279000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 280000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 281000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 282000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 283000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 284000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 285000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 286000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 287000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 288000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 289000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 290000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 291000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 292000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 293000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 294000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 295000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 296000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 297000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 298000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 299000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 300000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 301000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 302000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 303000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 304000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 305000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 306000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 307000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 308000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 309000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 310000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 311000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 312000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 313000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 314000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 315000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 316000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 317000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 318000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 319000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 320000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 321000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 322000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 323000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 324000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 325000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 326000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 327000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 328000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 329000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 330000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 331000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 332000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 333000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 334000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 335000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 336000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 337000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 338000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 339000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 340000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 341000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 342000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 343000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 344000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 345000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 346000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 347000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 348000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 349000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 350000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 351000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 352000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 353000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 354000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 355000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 356000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 357000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 358000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 359000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 360000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 361000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 362000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 363000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 364000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 365000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 366000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 367000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 368000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 369000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 370000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 371000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 372000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 373000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 374000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 375000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 376000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 377000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 378000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 379000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 380000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 381000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 382000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 383000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 384000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 385000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 386000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 387000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 388000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 389000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 390000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 391000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 392000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 393000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 394000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 395000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 396000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 397000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 398000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 399000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 400000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 401000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 402000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 403000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 404000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 405000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 406000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 407000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 408000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 409000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 410000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 411000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 412000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 413000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 414000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 415000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 416000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 417000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 418000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 419000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 420000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 421000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 422000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 423000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 424000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 425000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 426000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 427000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 428000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 429000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 430000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 431000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 432000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 433000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 434000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 435000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 436000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 437000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 438000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 439000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 440000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 441000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 442000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 443000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 444000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 445000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 446000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 447000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 448000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 449000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 450000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 451000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 452000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 453000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 454000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 455000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 456000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 457000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 458000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 459000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 460000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 461000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 462000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 463000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 464000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 465000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 466000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 467000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 468000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 469000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 470000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 471000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 472000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 473000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 474000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 475000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 476000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 477000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 478000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 479000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 480000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 481000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 482000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 483000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 484000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 485000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 486000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 487000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 488000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 489000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 490000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 491000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 492000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 493000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 494000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 495000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 496000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 497000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 498000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 499000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 500000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 501000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 502000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 503000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 504000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 505000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 506000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 507000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 508000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 509000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 510000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 511000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 512000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 513000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 514000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 515000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 516000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 517000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 518000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 519000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 520000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 521000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 522000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 523000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 524000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 525000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 526000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 527000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 528000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 529000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 530000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 531000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 532000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 533000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 534000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 535000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 536000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 537000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 538000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 539000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 540000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 541000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 542000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 543000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 544000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 545000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 546000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 547000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 548000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 549000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 550000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 551000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 552000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 553000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 554000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 555000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 556000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 557000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 558000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 559000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 560000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 561000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 562000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 563000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 564000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 565000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 566000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 567000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 568000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 569000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 570000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 571000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 572000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 573000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 574000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 575000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 576000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 577000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 578000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 579000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 580000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 581000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 582000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 583000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 584000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 585000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 586000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 587000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 588000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 589000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 590000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 591000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 592000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 593000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 594000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 595000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 596000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 597000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 598000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 599000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 600000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 601000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 602000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 603000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 604000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 605000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 606000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 607000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 608000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 609000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 610000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 611000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 612000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 613000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 614000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 615000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 616000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 617000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 618000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 619000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 620000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 621000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 622000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 623000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 624000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 625000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 626000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 627000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 628000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 629000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 630000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 631000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 632000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 633000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 634000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 635000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 636000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 637000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 638000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 639000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 640000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 641000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 642000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 643000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 644000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 645000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 646000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 647000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 648000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 649000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 650000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 651000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 652000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 653000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 654000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 655000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 656000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 657000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 658000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 659000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 660000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 661000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 662000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 663000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 664000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 665000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 666000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 667000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 668000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 669000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 670000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 671000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 672000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 673000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 674000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 675000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 676000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 677000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 678000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 679000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 680000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 681000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 682000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 683000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 684000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 685000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 686000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 687000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 688000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 689000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 690000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 691000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 692000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 693000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 694000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 695000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 696000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 697000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 698000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 699000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 700000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 701000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 702000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 703000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 704000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 705000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 706000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 707000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 708000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 709000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 710000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 711000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 712000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 713000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 714000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 715000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 716000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 717000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 718000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 719000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 720000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 721000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 722000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 723000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 724000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 725000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 726000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 727000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 728000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 729000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 730000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 731000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 732000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 733000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 734000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 735000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 736000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 737000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 738000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 739000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 740000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 741000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 742000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 743000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 744000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 745000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 746000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 747000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 748000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 749000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 750000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 751000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 752000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 753000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 754000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 755000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 756000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 757000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 758000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 759000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 760000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 761000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 762000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 763000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 764000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 765000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 766000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 767000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 768000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 769000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 770000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 771000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 772000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 773000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 774000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 775000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 776000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 777000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 778000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 779000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 780000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 781000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 782000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 783000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 784000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 785000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 786000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 787000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 788000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 789000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 790000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 791000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 792000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 793000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 794000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 795000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 796000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 797000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 798000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 799000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 800000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 801000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 802000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 803000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 804000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 805000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 806000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 807000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 808000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 809000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 810000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 811000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 812000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 813000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 814000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 815000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 816000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 817000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 818000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 819000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 820000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 821000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 822000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 823000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 824000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 825000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 826000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 827000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 828000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 829000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 830000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 831000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 832000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 833000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 834000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 835000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 836000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 837000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 838000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 839000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 840000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 841000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 842000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 843000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 844000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 845000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 846000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 847000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 848000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 849000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 850000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 851000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 852000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 853000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 854000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 855000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 856000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 857000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 858000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 859000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 860000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 861000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 862000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 863000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 864000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 865000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 866000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 867000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 868000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 869000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 870000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 871000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 872000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 873000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 874000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 875000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 876000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 877000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 878000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 879000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 880000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 881000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 882000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 883000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 884000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 885000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 886000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 887000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 888000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 889000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 890000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 891000/1000000 : Train accuracy 79.01234567901234%\n",
      "Loss is increasing, stop training at epoch 891576\n"
     ]
    }
   ],
   "source": [
    "r.train(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.44872295]]),\n",
       " array([[-6.82110647e-08],\n",
       "        [ 1.37139531e-08],\n",
       "        [ 1.23483610e-08],\n",
       "        [ 7.19484802e-10],\n",
       "        [ 4.84651579e-09]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 342)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.pred_class[r.pred_class > 0.5]),len(r.train_y[r.train_y == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(79.01234568)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.train_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = r.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class = np.where(res>=0.5,1,0)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 256)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[res > 0.5]),len(res[res < 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>EmbarkedClass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name  \\\n",
       "0            892       3                              Kelly, Mr. James   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                     Myles, Mr. Thomas Francis   \n",
       "3            895       3                              Wirz, Mr. Albert   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "..           ...     ...                                           ...   \n",
       "413         1305       3                            Spector, Mr. Woolf   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                           Ware, Mr. Frederick   \n",
       "417         1309       3                      Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \\\n",
       "0      male  34.5      0      0              330911    7.8292   NaN        Q   \n",
       "1    female  47.0      1      0              363272    7.0000   NaN        S   \n",
       "2      male  62.0      0      0              240276    9.6875   NaN        Q   \n",
       "3      male  27.0      0      0              315154    8.6625   NaN        S   \n",
       "4    female  22.0      1      1             3101298   12.2875   NaN        S   \n",
       "..      ...   ...    ...    ...                 ...       ...   ...      ...   \n",
       "413    male  28.0      0      0           A.5. 3236    8.0500   NaN        S   \n",
       "414  female  39.0      0      0            PC 17758  108.9000  C105        C   \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S   \n",
       "416    male  28.0      0      0              359309    8.0500   NaN        S   \n",
       "417    male  28.0      1      1                2668   22.3583   NaN        C   \n",
       "\n",
       "     EmbarkedClass  SexClass  Survived  \n",
       "0                1         1         0  \n",
       "1                2         0         0  \n",
       "2                1         1         0  \n",
       "3                2         1         0  \n",
       "4                2         0         1  \n",
       "..             ...       ...       ...  \n",
       "413              2         1         0  \n",
       "414              0         0         1  \n",
       "415              2         1         0  \n",
       "416              2         1         0  \n",
       "417              0         1         0  \n",
       "\n",
       "[418 rows x 14 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"Survived\"] = pred_class\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T10: My submission is\n",
    "test_df[[\"PassengerId\",\"Survived\"]].to_csv(\"submission.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T11: My submission score is 0.76076\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(degree=2,include_bias=False)\n",
    "poly_features_train=poly.fit_transform(train_data)\n",
    "poly_features_test=poly.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221378/758278441.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  def __sigmoid(self,z): return 1/(1+np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000000 : Train accuracy 71.8294051627385%\n",
      "Epoch 2000/1000000 : Train accuracy 62.177328843995504%\n",
      "Epoch 3000/1000000 : Train accuracy 77.32884399551067%\n",
      "Epoch 4000/1000000 : Train accuracy 42.98540965207632%\n",
      "Epoch 5000/1000000 : Train accuracy 79.01234567901234%\n",
      "Epoch 6000/1000000 : Train accuracy 71.60493827160494%\n",
      "Epoch 7000/1000000 : Train accuracy 79.34904601571269%\n",
      "Epoch 8000/1000000 : Train accuracy 79.2368125701459%\n",
      "Epoch 9000/1000000 : Train accuracy 76.99214365881033%\n",
      "Epoch 10000/1000000 : Train accuracy 74.5230078563412%\n",
      "Epoch 11000/1000000 : Train accuracy 72.16610549943884%\n",
      "Epoch 12000/1000000 : Train accuracy 72.50280583613917%\n",
      "Epoch 13000/1000000 : Train accuracy 64.53423120089786%\n",
      "Epoch 14000/1000000 : Train accuracy 64.42199775533109%\n",
      "Epoch 15000/1000000 : Train accuracy 58.13692480359147%\n",
      "Epoch 16000/1000000 : Train accuracy 78.56341189674522%\n",
      "Epoch 17000/1000000 : Train accuracy 75.42087542087542%\n",
      "Epoch 18000/1000000 : Train accuracy 63.07519640852974%\n",
      "Epoch 19000/1000000 : Train accuracy 79.46127946127946%\n",
      "Epoch 20000/1000000 : Train accuracy 69.80920314253648%\n",
      "Epoch 21000/1000000 : Train accuracy 68.12570145903479%\n",
      "Epoch 22000/1000000 : Train accuracy 75.98204264870931%\n",
      "Epoch 23000/1000000 : Train accuracy 77.21661054994388%\n",
      "Epoch 24000/1000000 : Train accuracy 79.12457912457911%\n",
      "Epoch 25000/1000000 : Train accuracy 62.96296296296296%\n",
      "Epoch 26000/1000000 : Train accuracy 80.13468013468014%\n",
      "Epoch 27000/1000000 : Train accuracy 79.57351290684625%\n",
      "Epoch 28000/1000000 : Train accuracy 68.01346801346801%\n",
      "Epoch 29000/1000000 : Train accuracy 78.11447811447812%\n",
      "Epoch 30000/1000000 : Train accuracy 79.91021324354658%\n",
      "Epoch 31000/1000000 : Train accuracy 66.10549943883277%\n",
      "Epoch 32000/1000000 : Train accuracy 65.76879910213243%\n",
      "Epoch 33000/1000000 : Train accuracy 78.33894500561168%\n",
      "Epoch 34000/1000000 : Train accuracy 66.10549943883277%\n",
      "Epoch 35000/1000000 : Train accuracy 65.31986531986533%\n",
      "Epoch 36000/1000000 : Train accuracy 61.05499438832772%\n",
      "Loss is increasing, stop training at epoch 36383\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression(poly_features_train,train_label,poly_features_test,0.003)\n",
    "model2.train(1000000)\n",
    "res2 = model2.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221378/758278441.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  def __sigmoid(self,z): return 1/(1+np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.52924676]]),\n",
       " array([[ 0.04398714],\n",
       "        [ 0.1066733 ],\n",
       "        [-0.04601717],\n",
       "        [ 0.64027905],\n",
       "        [ 0.05995578],\n",
       "        [ 0.29803205],\n",
       "        [-0.14987163],\n",
       "        [ 2.11926597],\n",
       "        [ 0.17642912],\n",
       "        [-0.04601717],\n",
       "        [-1.69215013],\n",
       "        [-0.08975483],\n",
       "        [-0.08944415],\n",
       "        [ 0.73012458],\n",
       "        [ 0.11209006]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 342)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.pred_class.sum(),len(model2.train_y[model2.train_y == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2_class = np.where(res2>=0.5,1,0)\n",
    "res2_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3. ,  1. , 34.5,  1. ],\n",
       "       [ 3. ,  0. , 47. ,  2. ],\n",
       "       [ 2. ,  1. , 62. ,  1. ],\n",
       "       ...,\n",
       "       [ 3. ,  1. , 38.5,  2. ],\n",
       "       [ 3. ,  1. , 28. ,  2. ],\n",
       "       [ 3. ,  1. , 28. ,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"Survived\"] = res2_class\n",
    "test_df[['PassengerId','Survived']].to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T12: My submission has acccuracy of model2 is worse than fisrt try\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000000 : Train accuracy 61.27946127946128%\n",
      "Epoch 2000/1000000 : Train accuracy 69.36026936026936%\n",
      "Epoch 3000/1000000 : Train accuracy 75.42087542087542%\n",
      "Epoch 4000/1000000 : Train accuracy 76.87991021324355%\n",
      "Epoch 5000/1000000 : Train accuracy 76.43097643097643%\n",
      "Epoch 6000/1000000 : Train accuracy 78.00224466891133%\n",
      "Epoch 7000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 8000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 9000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 10000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 11000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 12000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 13000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 14000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 15000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 16000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 17000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 18000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 19000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 20000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 21000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 22000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 23000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 24000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 25000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 26000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 27000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 28000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 29000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 30000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 31000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 32000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 33000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 34000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 35000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 36000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 37000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 38000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 39000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 40000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 41000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 42000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 43000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 44000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 45000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 46000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 47000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 48000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 49000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 50000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 51000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 52000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 53000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 54000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 55000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 56000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 57000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 58000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 59000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 60000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 61000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 62000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 63000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 64000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 65000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 66000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 67000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 68000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 69000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 70000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 71000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 72000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 73000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 74000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 75000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 76000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 77000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 78000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 79000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 80000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 81000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 82000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 83000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 84000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 85000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 86000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 87000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 88000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 89000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 90000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 91000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 92000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 93000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 94000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 95000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 96000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 97000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 98000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 99000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 100000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 101000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 102000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 103000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 104000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 105000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 106000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 107000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 108000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 109000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 110000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 111000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 112000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 113000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 114000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 115000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 116000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 117000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 118000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 119000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 120000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 121000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 122000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 123000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 124000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 125000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 126000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 127000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 128000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 129000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 130000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 131000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 132000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 133000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 134000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 135000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 136000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 137000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 138000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 139000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 140000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 141000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 142000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 143000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 144000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 145000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 146000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 147000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 148000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 149000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 150000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 151000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 152000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 153000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 154000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 155000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 156000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 157000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 158000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 159000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 160000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 161000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 162000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 163000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 164000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 165000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 166000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 167000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 168000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 169000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 170000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 171000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 172000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 173000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 174000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 175000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 176000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 177000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 178000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 179000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 180000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 181000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 182000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 183000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 184000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 185000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 186000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 187000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 188000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 189000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 190000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 191000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 192000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 193000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 194000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 195000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 196000/1000000 : Train accuracy 78.67564534231201%\n",
      "Epoch 197000/1000000 : Train accuracy 78.67564534231201%\n",
      "Loss is increasing, stop training at epoch 197830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.51473029]]),\n",
       " array([[-6.83343679e-08],\n",
       "        [ 3.55052215e-08],\n",
       "        [ 1.34485249e-09]]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(np.array(train_df[[\"SexClass\",\"Age\"]].values,dtype=np.float32).reshape(-1,2),train_label,np.array(test_df[[\"SexClass\",\"Age\"]].values,dtype=np.float32).reshape(-1,2),0.003)\n",
    "model3.train(1000000)\n",
    "model3.cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = model3.test()\n",
    "res3_class = np.where(res3>=0.5,1,0)\n",
    "test_df[\"Survived\"] = res3_class\n",
    "test_df[['PassengerId','Survived']].to_csv('submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T13: Seems a bit better?\n",
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
